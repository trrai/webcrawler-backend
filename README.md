# INFO 344 PA 3

## Website Link
[Link to working website](http://hw3cloud.cloudapp.net/)

## Screenshot
![alt text](https://i.imgur.com/42cG8I0.jpg)

## Write up
For this assignment, we were assigned to implement a web crawler that would be able to index the content posted by CNN and BleacherReport. Azure table and queue storage were crucial in the assignment and held all of the crawled data. The cloud application itself consists of a web and worker role. The web role is what the front end interacts with and delivers the appropriate information to the client. In addition, the web role communicates with the same queue and table storage that the worker role interacts with. The worker role is where the crawling and majority of work takes place. The process itself begins by going to the robots.txt file for each of the websites in order to fetch the sitemaps and crawling rules for CNN and BleacherReport. The sitemaps are sent into the queue and are ready to be processed one by one. During the parsing of a sitemap, the worker role inserts all of the urls that are new than two months into a separate queue. This process continues until all relevant sitemaps have been parsed and there around about 4-5 thousands links in the html link queue. When the queue with the sitemap is found to be empty, the worker role moves on to the next phase. At this point, the worker role will begin searching the html pages that were found from sitemaps, going through each site and finding stored urls within the website. This is done by using HTML Agility Pack and searching for `href` tags within the site. Any link that is found is then crosschecked with the list of already added links, and then inserted into the queue if the queue does not already contain it. In addition, there is a web request sent to the link in order to check if the website is up and running. In the case that a website returns a non 200 status code, it is sent to the errors table instead. I found that each site usually delivers 3-5 links contained within it that are inserted into the queue. Although, as you can assume, the number of links that are being added to the queue per second decreases overtime since the amount of duplicate collisions will increase. After all the links held within a website have been checked, the site is sent to the results table and is done with the crawling process. At the same time, the worker role retrieves an entity within the storage table that represents the count of the structure. After retrieving the current count, the worker role increments this count by one, and continues to replace the entity within the table. This means that there will always be one entity in the table that represents the count, and memory is not wasted. While crawling links, the worker role makes sure to only insert links that are NOT on the blacklist and are html pages. This is to ensure smooth operation and consistency within the table storage. Overall, the crawler itself was able to index about 315,000 links at the time of submission!

## Extra Credit Write up
For extra credit, I decided to implement the graphical dashboard in order to display visual indicators about the CPU% and memory usage. This was done by using a javascript library called ChartJs. I had to format the JSON information that was being delivered by the webrole in a way that would fit the requirements of the library. After getting the appropriate data, I was able to plot the past 10 entries on a graph with two fluid lines representing CPU% and RAM usage. In addition, there is an option that lets the user update the chart on request right above it. The RAM is plotted at a factor of x100 in order to have a similar scale to CPU%. 